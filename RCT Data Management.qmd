---
title: "RCT Data Management"
date: '`r Sys.Date()`'
toc: TRUE
embed-resources: true
output-location: column-fragment
---

# Introduction

Data Management code for the **S**creening, **S**elf-**M**anagement, and **R**eferral to **T**reatment (SSMRT) project.

This code accomplishes a few things:

-   Fetches data files (either locally or directly from REDCap)

-   Verifies participant IP addresses, compared against self-reported location

-   Various data quality checks

    -   Long-string analysis (also known as straight-lining)

    -   Speeding

    -   Inconsistency/attention checks

-   Publishes recruitment statistics

More detail for each of these steps can be found in their corresponding sections.

Contact the project administrators at [aloveroc\@ualberta.ca](mailto:aloveroc@ualberta.ca) and/or [jacob.belliveau\@dal.ca](mailto:jacob.belliveau@dal.ca) for the files and keys needed to run.

```{r data init}

library(quarto)

library(REDCapR)

# if the "keys.R" file does not exist, choose local filenames
if (!file.exists("keys.R")) {
  
  #baseline filename
  baseline_filename <- file.choose(new = TRUE)
  
  # intervention filenames
  i1_filename <- file.choose(new = TRUE)
  i2_filename <- file.choose(new = TRUE)
  
  # control filenames
  c1_filename <- file.choose(new = TRUE)
  c2_filename <- file.choose(new = TRUE)
  
} else {
  
  # running the file containing keys
  source("keys.R")
  
  # fetching baseline datasets
  baseline_df <- redcap_read_oneshot(DAL_URI, BASELINE_API_KEY)$data
  
  # fetching intervention datasets
  int1_df <- redcap_read_oneshot(DAL_URI, I1_API_KEY)$data
  int2_df <- redcap_read_oneshot(DAL_URI, I2_API_KEY)$data
  
  # fetching control datasets
  ctl1_df <- redcap_read_oneshot(DAL_URI, C1_API_KEY)$data
  ctl2_df <- redcap_read_oneshot(DAL_URI, C2_API_KEY)$data
  
  # clearing unneeded keys from the environment
  rm(BASELINE_API_KEY, I1_API_KEY, I2_API_KEY, C1_API_KEY, C2_API_KEY, DAL_URI)
  
}

```

# Data Linkage {#sec-data-linkage}

The data for this study consists of 5 main datasets: the baseline survey, and 2 datasets for each followup for both the control and intervention groups. The code below combines these datasets into one data set.

The follow-up datasets for control and intervention are combined into one data set, with the fields for each survey being assigned the suffix "\_F1" or "\_F2" to indicate variables that belong to follow-up 1 or 2, respectively. This results in two datasets (one for control, one for intervention) which each have the same sets of variables. The control and intervention datasets are then combined by appending the rows from one group into the other. As a final step, the baseline and follow-up datasets are then combined.

Note that matching at each applicable step is done using participant access codes. These access codes are assigned by the SSMRT platform and are unique to each participant. Thus, participants without access codes are discarded (as this is indicative of link tampering).

It is possible for access codes to be duplicated in any of the given 4 follow-up datasets. This is possible as participants are emailed the link to the survey; if they click that link multiple times, they will have multiple entries in the survey with the same access codes. For simplicity, we consider only the "latest" entry into the survey as valid for duplicate participants; other attempts are discarded.

In the event that a participant completes the second follow-up, but does not begin/complete the first follow-up, a blank row is added to follow-up 1 with their access code.

```{r data linkage}

library(tidyr)
library(dplyr)

### Functions used in this section:
`%notin%` <- Negate(`%in%`) # opposite of the %in% operator

### first combining into wide format

# first correcting the final variable of the dataset (binary variable indicating if they finished the survey; different name for each survey)

ctl1_df <- rename(ctl1_df, 
       followup_complete = rct_follow_up_1_control_group_complete)
ctl2_df <- rename(ctl2_df, 
       followup_complete = rct_follow_up_2_control_group_complete)

int1_df <- rename(int1_df, 
       followup_complete = rct_follow_up_1_intervention_group_complete)
int2_df <- rename(int2_df, 
       followup_complete = rct_follow_up_2_intervention_group_complete)

### The section below handles participants who complete the follow-up 2 survey, but not follow-up 1. Without this section, these participants would be lost at the full-join operation below. 

## Control group
# getting a list of unique access codes in follow up 2 of the control group
ctl_pt2_acs <- unique(ctl2_df$access_code[!is.na(ctl2_df$access_code)])

# checking if all access codes in F2 are in F1. If they are not...
if (!all(ctl_pt2_acs %in% ctl1_df$access_code)) {
  # for each access code missing from F1...
  for (AC in ctl_pt2_acs[ctl_pt2_acs %notin% ctl1_df$access_code]) {
    # add a blank/NA row to F1
    ctl1_df[nrow(ctl1_df)+1,] <- NA
    # add in the missing access code to the empty row
    ctl1_df[nrow(ctl1_df),4] <- AC
    # removes the access code from the environment
    rm(AC)
  }

}

## Intervention group
# getting a list of unique access codes in follow up 2 of the control group
int_pt2_acs <- unique(int2_df$access_code[!is.na(int2_df$access_code)])

# checking if all access codes in F2 are in F1. If they are not...
if (!all(int_pt2_acs %in% int1_df$access_code)) {
  # for each access code missing from F1...
  for (AC in int_pt2_acs[int_pt2_acs %notin% int1_df$access_code]) {
    # add a blank/NA row to F1
    int1_df[nrow(int1_df)+1,] <- NA
    # add in the missing access code to the empty row
    int1_df[nrow(int1_df),4] <- AC
    # removes the access code from the environment
    rm(AC)
  }

}

# combining control follow-ups into one dataset
control <- full_join(ctl1_df, ctl2_df, by = "access_code", suffix = c("_F1", "_F2"), na_matches = "never")

# combining intervention follow-ups into one dataset
intervention <- full_join(int1_df, int2_df, by = "access_code", suffix = c("_F1", "_F2"), na_matches = "never")

# combining the intervention and control datasets
followup <- rbind(control, intervention)

# combining the follow-up surveys to baseline
# NOTE: here, participants without access codes in the follow-up are discarded
# NOTE: the "latest" response is kept for participants with duplicate access codes
df <- left_join(baseline_df, followup, by = "access_code", na_matches = "never", multiple = "last")

# removing temporary variables ; df will be used going forward
rm(control, intervention, followup, int1_df, int2_df, ctl1_df, ctl2_df, baseline_df, ctl_pt2_acs, int_pt2_acs)

# variable for writing to file later
RAW_DATA_TO_SAVE <- df
```

# Participant validation

These participants are either automatically flagged for exclusion, or flagged for manual inspection by the research team. Once a decision is made for these participants, the decision is recorded in the code below. In general, participants who show attempts of trying to game the survey for extra compensation are excluded, and other genuine attempts are retained (e.g., participants who began the survey but did not complete it and restarted later, with responses being roughly consistent for each attempt). Signs of "gaming" the survey that were taken into consideration by the research team include:

-   Attempting the survey using multiple email addresses (or the same email address or phone number)

-   Providing different demographic information across attempts

-   Failing the screening questions on the initial attempt, and re-trying the survey with different responses to the screening questions

## Duplicate participants

The code below creates a variable (manual_check), flagging participants for manual inspection if they provided an email, phone number, access codes, or IP addresses. The research team discusses any participants who are flagged by these checks, and once a decision is reached, these decisions are recorded in a CSV file. The code then checks this list, and creates a second variable (manual_decision) un-flagging any participants who have already had a decision made.

The file containing decisions has 3 columns: [access_code]{.underline}, [record_id]{.underline}, and [decision]{.underline}. These columns are used as follows:

-   access_code: Always to be filled out (if available). Can be the sole identifier if the corresponding decision applies to all attempts by this participant (i.e., if we're excluding all attempts from this participant).

-   record_id: Needed in conjunction with access_code when you are specifying to keep 1 attempt from a participant. All other attempts will be flagged for removal. Can also be used to identify single rows (sans access_code) to be excluded.

-   decision: Single-character column containing decisions, represented by either the letter "k" (for "keep") or "e" (for "exclude")

```{r duplicate}

library(openxlsx)

## Getting a vector of duplicate email addresses
# in "email"
  # getting duplicates 
duplicate_email <- unique(df$email[duplicated(df$email)])
  # removing NAs
duplicate_email <- duplicate_email[!is.na(duplicate_email)] 

#in "email_end"
  # getting duplicates 
duplicate_email_end <- unique(df$email_end[duplicated(df$email_end)]) 
  # removing NAs
duplicate_email_end <- duplicate_email_end[!is.na(duplicate_email_end)]


# combining the above 2 vectors
duplicate_emails <- unique(c(duplicate_email, duplicate_email_end))
# removing unneeded vectors
rm(duplicate_email, duplicate_email_end)

## Getting a vector of duplicate access codes and removing NAs
duplicate_access_code <- unique(df$access_code[duplicated(df$access_code)])
duplicate_access_code <- duplicate_access_code[!is.na(duplicate_access_code)]

## Getting a vector of duplicate IP addresses
  # reading in IP address file
IPs <- read.xlsx("SSMRT IPs.xlsx", sheet = 1)

  # finding duplicates (there are no NAs in this file)
duplicate_ips <- unique(IPs$ip[duplicated(IPs$ip)])


## Flagging duplicates

# initalizing variable
df$manual_check <- 0

# flagging those with duplicate emails in "email" or "email_end"
df$manual_check[df$email %in% duplicate_emails | 
                  df$email_end %in% duplicate_emails] <- 2

# flagging those with duplicate access codes
df$manual_check[df$access_code %in% duplicate_access_code] <- 2

# flagging those with duplicate IP addresses
# this one is a little different - as the IP addresses aren't in the main dataframe, we instead check for the record IDs from the IPs dataframe and then flag those record IDs in the main dataframe
df$manual_check[df$record_id %in% (IPs$record_id[IPs$ip %in% duplicate_ips])] <- 2

## Checking decisions

# initializing variable
df$manual_decided <- df$manual_check

# reading in the file recording decisions
decisions <- read.csv("Manual Decisions.csv", header = TRUE)

# handling decisions made solely with access codes (applied to all attempts)
for (id in decisions$access_code[is.na(decisions$record_id)]) {
  
  if (!is.null(id)) { # if "id" is not NULL
    
    if (decisions$decision[decisions$access_code == id] == "e") { # if the decision is to exclude
      
      df$manual_decided[df$access_code == id] <- 1 #exclude
    
      } else {
      df$manual_decided[df$access_code == id] <- 0 #otherwise, keep
    }
    
  }
  rm(id)
}

# handling decisions made solely with record IDs (applied to single rows)
for (id in decisions$record_id[decisions$access_code == ""]) {
  
  if (!is.null(id)) { # if "id" is not NULL
    
    if (decisions$decision[decisions$record_id == id & 
                           !is.na(decisions$record_id)] == "e") { # if the decision is to exclude
      
      df$manual_decided[df$record_id == id] <- 1 #exclude
    
      } else {
      df$manual_decided[df$record_id == id] <- 0 #otherwise, keep
    }    
    
  }
  rm(id)
}

# handling decisions made with both access codes and IDs
for (id in decisions$access_code[decisions$access_code != "" & 
                                 !is.na(decisions$record_id)]) {
  
  if (!is.null(id)) { # if "id" is not NULL
    
    # fetching record id corresponding to access code "id" 
    rid <- decisions$record_id[decisions$access_code == id]
    
    # exclude all participants with that access code
    df$manual_decided[df$access_code == id] <- 1 
    # un-exclude participants with that access code and the record id fetched
    df$manual_decided[df$access_code == id & df$record_id == rid] <- 0
    
  }
  rm(id)
}

rm(IPs,id,decisions,duplicate_access_code,duplicate_emails,duplicate_ips)

```

## Participants with invalid R/S pairs

In the initial study link sent to participants, there are two unique identifiers provided to participants: R and S values. These identifiers are generated by the research team and consist of randomly generated patterns of letters and numbers. Specifically, the **R values** are 3 characters long and can only contain the numbers 1 to 9 (not 0) and the letters A, B, C, D, E, F, G, H, I , J, K, X, and Z. **S values** can contain any letter or number and are 10 characters long. At project initialization, the research team generated a list of 10,000 of each of these values. Each list of values was added to a spreadsheet, and it was recorded which email/participant was sent the link containing the pair of R/S identifiers. Thus, each pair of identifiers is unique and traceable to each participant. The code below serves to identify that 1) each R/S pair is unique (i.e., there are no duplicate participant entries), and 2) that each R/S pair is valid compared against the pre-generated list of R/S pairs.

These participants are considered to be gaming the system without exception. This includes those with blank or undefined R/S values. These participants are unilaterally excluded from the final data set.

```{r validation}

library(openxlsx)
library(stringr)

### Functions used
# %notin% described in section "data linkage"


### Validating codes
# Reads the XLSX file of R/S code pairs used in the study. This list does not change and contains 10,000 code pairs.
validpairs <- read.xlsx("SSMRT RS Values.xlsx", sheet = 1)

# concatenates the S and R values into one variable in both the R/S code pairs dataset and the REDCap dataset
validpairs$codes <- str_c(validpairs$S.Codes, validpairs$R.Codes)
df$codes <- str_c(df$schlesingers, df$schlesingerr)

df$invalid_code <- 0 #creates a variable named "invalid pairs" with values 0
df$invalid_code[df$codes %notin% validpairs$codes # flags codes that are not in the validpairs dataset
                 & !is.na(df$codes) # and ignores NA values
                 ] <- 1 # flagged with value "1"

### Blank codes
df$blank_code <- 0 # creates a variable for flagging those with empty codes
# flags those with missing R or S values (or both) by checking that the "codes" variable is 13 characters long, or is equal to "undefinedundefined"
df$blank_code[nchar(df$codes) != 13 | 
                is.na(df$codes)     | 
                df$codes == "undefinedundefined"] <- 1 

### Removing the $codes variable as it's no longer needed
df$codes <- NULL

### Removing the validpairs dataframe from the environment
rm(validpairs)

```

## Duplicate access codes

Duplicate access codes in the follow-up surveys are handled in [Data Linkage](#sec-data-linkage). Duplicate access codes at the baseline are handled below. This is not something which should happen often and is indicative of participants copying the initial survey link once they reach the screening questions. These participants are considered to be gaming the system, and are flagged for removal.

```{r duplicate acs}

# getting a vector of duplicate access codes
duplicated_access_codes <- unique(df$access_code[duplicated(df$access_code)])
# removing NA from the above vector
duplicated_access_codes <- duplicated_access_codes[!is.na(duplicated_access_codes)]

df$ac_duplicate <- 0
# flagging those who have a duplicate access code
df$ac_duplicate[df$access_code %in% duplicated_access_codes] <- 1

# removing unneeded environment objects
rm(duplicated_access_codes)

```

# Data quality checks

The sections below deal with various aspects of checking for data quality. The primary goal is verifying that participants are genuine and attentive.

## Participant location

Participant IP addresses are collected as they complete the baseline survey. Using [IPinfo](https://ipinfo.io/) and custom functions, participant province as reported by their IP address is compared against their self-reported province of residence. Participant province is cached locally.

If you are running this script once data collection is complete, only cached data will be used and no integration with IPinfo is needed.

Comparison between self-reported and IP location is handled in [Inconsistency checks].

```{r ip}

### Functions used
# Author-created function interfacing with IPInfo
ipdat <- function(ip_addresses, token) {
  # Checking for the HTTR package
  if (!require("httr", character.only = TRUE, quietly = TRUE)) {
    stop("Required package 'httr' is not installed. Please install before using this function.")
  }
  # Checking for the jsonlite package
  if (!require("jsonlite", character.only = TRUE, quietly = TRUE)) {
    stop("Required package 'jsonlite' is not installed. Please install before using this function.")
  }
  
  # Validating token
  validateToken <- httr::GET(paste0("https://ipinfo.io?token=", token))
  
  # If token is invalid, stop
  if (httr::status_code(validateToken) != 200) {
    httr::status_code(validateToken)
    stop("Invalid API token, or problems with the service.")
  }
  
  # Generating endpoint for API
  endpoint <- paste0("https://ipinfo.io/batch?token=", token)
  
  # converting IP address list to JSON
  payload <- jsonlite::toJSON(ip_addresses)
  
  # Getting IP data
  response <- httr::POST(endpoint, body = payload, content_type("application/json"))
  
  # Selecting and returning only content; stops if the response failed
  if (httr::http_type(response) == "application/json") {
    data <- httr::content(response, as = "text")
    parsed_data <- jsonlite::fromJSON(data)
    
    return(parsed_data)
  } else {
    stop("API request failed")
  }
}


# Function for extracting info from a list of lists
# Complement to the above function
# Note that returned columns are coerced into character types
unlistLists <- function(listNest, data_points) {
  # Extract the relevant data points from each IP info entry
  extracted_data <- lapply(listNest, function(entry) {
    extracted_entry <- lapply(data_points, function(point) {
      if (point %in% names(entry)) {
        entry[[point]]  # Extract the data point from the entry if it exists
      } else {
        NA_character_  # Fill missing values with NA
      }
    })
    extracted_entry <- as.list(extracted_entry)
    if (any(!is.na(unlist(extracted_entry)))) extracted_entry else NULL  # Filter out empty entries
  })
  
  # Filter out empty entries
  non_empty_entries <- extracted_data[lengths(extracted_data) > 0]
  
  if (length(non_empty_entries) > 0) {
    # Combine non-empty entries into a data frame and convert list columns to separate columns
    df <- as.data.frame(do.call(rbind, non_empty_entries), stringsAsFactors = FALSE)
    df <- as.data.frame(lapply(df, as.character), stringsAsFactors = FALSE)
    colnames(df) <- data_points
    rownames(df) <- seq_len(nrow(df))
  } else {
    # Create an empty data frame with appropriate column names
    df <- data.frame(matrix(ncol = length(data_points)))
    colnames(df) <- data_points
  }
  
  return(df)
}


### Packages used
library(openxlsx)
library(rlang)


### IP Info

IPs <- read.xlsx("SSMRT IPs.xlsx", sheet = 1) # reading IP address file

if (file.exists("SSMRT Location Data.xlsx")) { # if the location data file exists
  LOCATIONS <- read.xlsx("SSMRT Location Data.xlsx", sheet = 1) # read it
} else { # if not
  LOCATIONS <- data.frame( # create a blank data frame to fill and save later
    region = character(), # with region
    id = character() # and id columns
  )
}

# seeing which IPs in the IP file have no corresponding location data in the location file
unfetched_IPs <- IPs$record_id[!(IPs$record_id %in% LOCATIONS$id)]

if (!is_empty(unfetched_IPs)) { # if there is at least 1 IP address
  for (id in unfetched_IPs) { # for each id
    ip <- IPs$ip[IPs$record_id == id] # get the corresponding IP address
    loc <- unlistLists( # (returned as a list so this unlists)
      ipdat(ip, IPINFO_API_TOKEN), # fetch location data
      c("region")) # keeping only the region column
    loc$id <- id # adding ip address to the region
    LOCATIONS <- rbind(LOCATIONS, loc) # binding this row of data to the larger location data file
  }
  rm(loc,ip,id) # removing unneeded data used in this loop
}

# combining the location data into the larger dataset, joining by record ID
df <- left_join(
  df, LOCATIONS, by = c("record_id" = "id")
)

# removing unneeded environment objects
rm(IPINFO_API_TOKEN, ipdat, unlistLists, unfetched_IPs, IPs)
```

## Speeding

Participants are checked for speeding at the baseline. (Do we want to check at each step?) That is, participants who complete the survey too quickly are flagged.

What is "too quickly"? We have defined it as being more than one third the median time. The citation for this cutoff can be found [here](https://doi.org/10.1002/9781118763520.ch11).

```{r speeding}

### Packages
library(lubridate)


### Detecting speeding
# creating a column with the time difference between the start time and end time
df$completetime <- difftime(df$exit_timestamp, df$entrance_timestamp)

# calculating a cutoff (1/3 median of the above)
cutoff <- median(df$completetime[!is.na(df$completetime)]) * 0.3
cutoff # displaying cutoff

df$speeder <- 0 # creating a "speeder" column initialized to 0
df$speeder[df$completetime <= cutoff] <- 1 # setting to 1 for those where the complete time is less or equal to than the median

# removing unneeded environment objects
rm(cutoff)

```

## Inconsistency checks

The code chunks below assess various inconsistency checks. These variables include questions where participants are expected to respond a certain way based on previous survey responses. The variables checked include:

-   cudit_1 : participants should never report "Never" using cannabis, as per the inclusion criteria

-   cudit_check : this is a duplicate of cudit_1, so the same criteria are checked. It is also checked that cudit_1 and cudit_check are equal (as they are identical questions)

-   can_freq_3mon : participants should not report no past 3-month cannabis use

-   can_3month and can_6month : participants should report past 3- and 6-month cannabis use, as per the inclusion criteria

-   age : participant self-reported age should align with their self-reported birth year/month (with some flexibility to account for not having the specific birth date)

-   location : participant self-reported province should align with their location reported by their IP address, detailed in [Participant location].

Each of these checks has their own indicator variable, with a value of 0 indicating "pass" and a value of 1 indicating "fail"

```{r cudit1 incon}

df$incon_cudit1 <- 0
# checks that cudit_1 and cudit_check are equal to 1; flags those who are not
df$incon_cudit1[df$cudit_1 == 0] <- 1
df$incon_cudit1[df$cudit_check == 0] <- 1

```

```{r cudit check incon}

df$incon_cuditcheck <- 0
# checks that cudit_1 and cudit_check match
df$incon_cuditcheck[df$cudit_1 != df$cudit_check & !is.na(df$cudit_1) & !is.na(df$cudit_check)] <- 1

```

```{r can_freq_3mon incon}

df$incon_canfreq <- 0
# checks that can_freq_3mon is not equal to 0
df$incon_canfreq[df$can_freq_3mon == 0] <- 1

```

```{r can_3month incon}

df$incon_can3mon <- 0
# checks that can_3month and can_6mon are not equal to 0
df$incon_can3mon[df$can_3month == 0 | df$can_6mon == 0] <- 1

```

```{r age incon}

# Packages used
library(lubridate)

# fixing the date of birth (dob) variable (this already existed in the dataset)
df$dob <- str_c(df$dob_year, "-", df$dob_month, "-01")
# fixing the calculated age, rounding to nearest full number (this variable also already existed)
df$zage_calc <- round(
  as.numeric(
    difftime(df$today, df$dob, units = "weeks")
    , units = "weeks")
  /52.18 # dividing as difftime units only go up to weeks; converting to years
)

df$incon_age <- 0 # initializing inconsistency variable as being "0"
# if self-reported age and calculated age are not equal (with a margin of 1)
df$incon_age[df$age_screen+1 != df$zage_calc 
             & df$age_screen != df$zage_calc] <- 1 # set to 1

```

```{r location incon}

# creating a temporary column of province labels based on the numeric "province_screen"
df$province_screen_label <- recode(df$province_screen, 
                                   '1' = 'British Columbia',
                                   '2' = 'Alberta',
                                   '3' = 'Saskatchewan',
                                   '4' = 'Manitoba',
                                   '5' = 'Ontario',
                                   '6' = 'Quebec',
                                   '7' = 'New Brunswick',
                                   '8' = 'Nova Scotia',
                                   '9' = 'Prince Edward Island',
                                   '10' = 'Newfoundland')

df$incon_province <- 0
# checks that region and labelled variable above match, and that neither variable is blank (NA)
df$incon_province[df$region != df$province_screen_label & !is.na(df$province_screen_label) & !is.na(df$province_screen)] <- 1

df$province_screen_label <- NULL # clearing label column as it's no longer needed

```

## Attention checks

The code below is checking the one direct attention check in the study, which asks participants to select "Several Days" (1) as the answer for that question.

```{r select c}

df$attncheck_fail <- 0
df$attncheck_fail[df$attncheck %in% c(0, 2, 3)] <- 1

```

## Long-string analysis

Long-string analysis (also known as straight-lining) is checked using a custom function. Each range of variables will have its own column created, with a value of TRUE indicating straight-lining present and FALSE indicating no straight-lining. Note that the function defined also has the ability to ignore missing values and any specified values (e.g., system missing). In this case, we will ignore only missing values.

Generally, we exclude participants who are flagged for straight-lining on more than one third of measures. In this case, since there are 5 measures being verified, we are flagging for removal those who straight-lined on more than 2 measures.

There are several variables created here: one flag variable for each range checked, a summary variable summing the number of flags, and a final summary flag for highlighting participants who straight-lined on more than 4 measures.

```{r longstring}

### Functions used
SL.check <- function(df, vars, new_col, ignore.na = TRUE, ignore.value = c(), ignore.vars = c()) {
  # Check if vars are in the data frame
  if (!all(vars %in% colnames(df))) {
    stop("Error: one or more of the specified variables is not in the data frame.")
  }
  
  # finding the first and last indices of the vars variable
  firstCol <- which(colnames(df)==vars[1])
  lastCol <- which(colnames(df)==tail(vars,1))
  
  # getting the names of columns between both indices
  namesofcols <- names(df)[firstCol:lastCol]
  
  # if ignore.vars is not empty, remove those names from the namesofcols variable
  if (length(ignore.vars > 0)) {
    namesofcols <- namesofcols[!(namesofcols %in% ignore.vars)]
  }
  # Check if the variables have the same value
  if (ignore.na) {
    # Ignore missing values if ignore.na is TRUE
    same_value <- !apply(df[,namesofcols], 1, function(x) any(is.na(x)))
  } else {
    # Don't ignore missing values if ignore.na is FALSE
    same_value <- !apply(df[,namesofcols], 1, function(x) any(is.na(x)))
  }
  
  # If ignore.value is not empty, check if all values are NOT in ignore.value
  if (length(ignore.value) > 0) {
    same_value <- same_value & !apply(df[,namesofcols], 1, function(x) any(x %in% ignore.value))
  }
  
  # Check if all pairs of variables are equal
  same_value <- same_value & apply(df[,namesofcols], 1, function(x) all(x == x[1]))
  
  # Add the same_value vector as a new column in the data frame
  df[,new_col] <- same_value
  
  # Return the modified data frame
  df
}

df <- SL.check(df, c("pbsm_sf_1", "pbsm_sf_13"), "SL_pbsm")
df <- SL.check(df, c("snp_1", "snp_7"), "SL_snp")
df <- SL.check(df, c("gad_1", "gad_7"), "SL_gad", ignore.vars = c("attncheck"))
df <- SL.check(df, c("phq_1", "phq_8"), "SL_phq")
df <- SL.check(df, c("k6_01", "k6_06"), "SL_k6")

# Add a new column called "SL.total" to the data frame
first_SL <- which(colnames(df) == "SL_pbsm")
last_SL <- which(colnames(df) == "SL_k6")

df$SL_total <- rowSums(df[first_SL:last_SL])

# Creates a flag which is 0 if they pass, and 1 if participants were flagged
# for straightlining more than 4 times.
df$SL_flag <- 0
df$SL_flag[df$SL_total > 2] <- 1

# removing unneeded environment objects
rm(SL.check, first_SL, last_SL)


```

# Exclusion summary

The code below generates a summary variable, flagging participants for exclusion based on the variables generated above. For this project, the team has opted to flag for exclusion participants who fail any given data quality check. Note that this also flags those who chose to opt out of participating in the study.

```{r exclude}

df$exclude <- 0
df$exclude[
  df$manual_decided == 1
  | df$invalid_code == 1
  | df$blank_code == 1
  | df$speeder == 1
  | df$incon_cudit1 == 1
  | df$incon_cuditcheck == 1
  | df$incon_canfreq == 1
  | df$incon_can3mon == 1
  | df$incon_age == 1
  | df$incon_province == 1
  | df$attncheck_fail == 1
  | df$SL_flag == 1
  | df$ac_duplicate == 1
  | df$withdraw___1 == 1
] <- 1

## TODO:
# check that this actually works - currently, everyone is flagged

```

# Report generation

The code below generates a report of relevant recruitment statistics and creates necessary files, as well as preparing the information to be sent to Google Sheets for tracking during recruitment.

If you are running this code once recruitment is complete, the reports requiring Google Authentication will be skipped. The tables and other demographic information that would be posted to Google Sheets and/or appended to other reports will be rendered below.

## Placeholder report name

Report.

```{r report name}



```

## Google reporting

Reporting to Google Sheets.

```{r google reporting}

## TODO: 
# Have a file tracking recruitment numbers over time (for charting)
# Have a pie chart of gender and province
# Basic demographics (age, income)

# if the recruitment number file exists
if (file.exists('N.csv')) {
  # reading recruitment numbers file
  RECRUITMENT_NUMBERS <- read.csv('N.csv', header = TRUE)
  
  # if today isn't in the recruitment numbers file
  if (any(RECRUITMENT_NUMBERS$date %notin% as.character(Sys.Date()))) {
    # creating a new single-row data frame with date and N
    newN <- data.frame(date = as.character(Sys.Date()),
                       N = length(df$record_id[df$exclude == 0])
    )
    # binding to existing data
    RECRUITMENT_NUMBERS <- rbind(RECRUITMENT_NUMBERS, newN)
    rm(newN)
  }
  
  # if today IS in the file
  if (any(RECRUITMENT_NUMBERS$date %in% as.character(Sys.Date()))) {
    # updates today's N with current N
    RECRUITMENT_NUMBERS$N[RECRUITMENT_NUMBERS$date == Sys.Date()] <- length(df$record_id[df$exclude == 0])
  }
}

```

## Output files

All output files created by any part of code above are saved here.

```{r output}

# Checks for the existence of a folder in the project directory named "output" - if it does not exist, creates it.
if (!dir.exists("output")) {dir.create("output")}

# Creates the folder name for today's output
todays_folder_name <- paste("output", Sys.Date(), "", sep = "/")

# if the above folder doesn't exist, creates it
if (!dir.exists(todays_folder_name)) {dir.create(todays_folder_name)}

# writing raw data to file
write.csv(RAW_DATA_TO_SAVE, paste(todays_folder_name, "SSMRT rawData.csv", sep = ""), row.names = FALSE)

# writing location data to file
write.xlsx(LOCATIONS, "SSMRT Location Data.xlsx")

# writing N tracking to file
write.csv(RECRUITMENT_NUMBERS, "N.csv", row.names = F)

END_REACHED <- TRUE
```
